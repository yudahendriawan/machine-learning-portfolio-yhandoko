{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOukyDp9iFM6XXwEZPHU4lH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yudahendriawan/google-colab-projects/blob/general/portfolio_sentiment_analysis_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Sentimen Analysis with Deep Learning using IndoNLU datasets"
      ],
      "metadata": {
        "id": "Hn-EiWtOgFCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "4CzBqa6hhEso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/indobenchmark/indonlu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWFk011afvlS",
        "outputId": "933f70c4-187c-4141-eaa5-c585bdd410a0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'indonlu' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets take a look to the github folder that we have cloned. There is indonlu folder > dataset > smsa_doc_sentiment_prosa. In that folder our data is located. We can see several files such as train_preprocess.tsv (this is the training data that we will use), and valid_preprocess.tsv (this is the validation data that we will use)."
      ],
      "metadata": {
        "id": "dAQzifbagQsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from indonlu.utils.forward_fn import forward_sequence_classification\n",
        "from indonlu.utils.metrics import document_sentiment_metrics_fn\n",
        "from indonlu.utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader"
      ],
      "metadata": {
        "id": "FI2Q5JU-f3AG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set and assign random seeds.\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Count the number of parameter in models\n",
        "def count_param(module, trainable=False):\n",
        "    if trainable:\n",
        "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "    else:\n",
        "        return sum(p.numel() for p in module.parameters())\n",
        "\n",
        "# Set learning rate\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "# Convert metrics into a string\n",
        "def metrics_to_string(metric_dict):\n",
        "    string_list = []\n",
        "    for key,value in metric_dict.items():\n",
        "        string_list.append('{}:{:.2f}'.format(key,value))\n",
        "    return ' '.join(string_list)"
      ],
      "metadata": {
        "id": "bqpSu8vYg8-B"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(19072021)"
      ],
      "metadata": {
        "id": "htcpNk91iKOZ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of setting the random seed is so that the model gives the same results every time we carry out the training process. To make things easier, set random_seed to the date when we run the code."
      ],
      "metadata": {
        "id": "MoSv0KXZZukU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Pre-trained Model and Configuration"
      ],
      "metadata": {
        "id": "TBpYB3SYiTgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, we use the pre-trained Indobert-base-p1 model which has 124.5 million parameters\n",
        "\n",
        "The Indobert model is built based on the general-purpose architecture BERT (Bidirectional Encoder Representation from Transformers). BERT was designed to help computers understand the meaning of ambiguous language in text. The trick is to use the surrounding text to build context."
      ],
      "metadata": {
        "id": "kqlAT00Eicxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load tokenizer and config\n",
        "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config.num_labels = DocumentSentimentDataset.NUM_LABELS\n",
        "\n",
        "#instantiate model\n",
        "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6uHHsgiNtc",
        "outputId": "c45b286d-48fa-4193-ace3-7e7a8b29f65a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwZ9U_HOjFnq",
        "outputId": "644dccf7-d84a-4087-8a37-3ecfc063547e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_param(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAT2O8TojNGO",
        "outputId": "3ef15128-1f2a-4ab2-f240-138953eacf68"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124443651"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation"
      ],
      "metadata": {
        "id": "ZXrUWYnZjfeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv'\n",
        "valid_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv'\n",
        "test_dataset_path = '/content/indonlu/dataset/smsa_doc-sentiment-prosa/test_preprocess_masked_label.tsv'"
      ],
      "metadata": {
        "id": "lmcYHK93jVOL"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After determining the location of the dataset, we need to prepare the data with Pytorch. PyTorch provides a standardized way to prepare data before modeling. PyTorch provides many advanced features for processing data.\n",
        "\n",
        "Here, we will use 2 classes provided in PyTorch in torch.utils.data module namely Dataset and DataLoader. Adapted from Pre-Trained Models for NLP Tasks Using PyTorch, the Dataset class is an abstract class that we need to extend in PyTorch. Meanwhile, DataLoader is the core of the data processing toolkit in PyTorch. DataLoader provides a wealth of functionality for preparing data including various sampling methods, parallel computing, and distributed processing. Well, we will move objects from Dataset class into objects from DataLoader class for further batch processing of data."
      ],
      "metadata": {
        "id": "noI2GoxakAMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\n",
        "valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
        "test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\n",
        "\n",
        "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512,\n",
        "                                           batch_size=32, num_workers=16, shuffle=True )\n",
        "valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512,\n",
        "                                           batch_size=32, num_workers=16, shuffle=True )\n",
        "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512,\n",
        "                                          batch_size=32, num_workers=16, shuffle=True )"
      ],
      "metadata": {
        "id": "8D0AnRcDFUh5"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxPdosqyGu14",
        "outputId": "d4553938-1e38-4bee-f134-0a7e24c223f2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([    2,  6540,    92,  2970,   213,  4259,  3553,   899,    34,\n",
            "         259,  5590,   262,  2558,   386,   899,  1687,    26,  1574,\n",
            "       30470,   899,  3310, 30468, 22130, 30360,  6123,  6368, 30468,\n",
            "       22130, 30360,  2652,  1746, 30468,  8869,  6540,    34,  6315,\n",
            "        1622,  1256,  8949,   899, 30468,  4222,  1622,   752,   245,\n",
            "         295,  2083, 30470,  2346,  7107,   300, 30470,   405,   724,\n",
            "        5189, 30470,   843, 17464,   899,   540, 10989,  3331,  1107,\n",
            "       30468,   119,  3221,    79,    34,  2170,    98,  9167, 30457,\n",
            "           3]), array(0), 'warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to define variables, for example w2i and i2w to place DocumentSentimentDataset.LABEL2INDEX and DocumentSentimentDataset.INDEX2LABEL."
      ],
      "metadata": {
        "id": "QD1mFijkHHsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
        "print(w2i)\n",
        "print(i2w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLsolKThG9E3",
        "outputId": "e2107e4a-80ba-456c-b7b7-6535113ea63c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'positive': 0, 'neutral': 1, 'negative': 2}\n",
            "{0: 'positive', 1: 'neutral', 2: 'negative'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model with Example"
      ],
      "metadata": {
        "id": "rs5j2ONuHqfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ0Qh1B0HUwQ",
        "outputId": "b45dbf28-701d-42d0-9c81-209cdafca8eb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita | Label : positive (39.380%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the evaluation above, it only has 39% positive sentiment. In fact, we can identify that the text should have positive sentiment more than 50% and even more than 90%. Therefore, let's carry out the Fine Tuning and Evaluation process."
      ],
      "metadata": {
        "id": "R45vgGAbaM7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning and Evaluation"
      ],
      "metadata": {
        "id": "kiIU6yucITeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "9UdpAVe2IJOL"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will train the model with the number of epochs = 5. The steps we will carry out include:\n",
        "\n",
        "1. Model training and updating process\n",
        "2. Training metrics calculation\n",
        "3. Evaluation on validation data\n",
        "4. Validation matrix calculation"
      ],
      "metadata": {
        "id": "2ulA8osvI1Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "n_epochs = 5\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    total_train_loss = 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
        "    for i, batch_data in enumerate(train_pbar):\n",
        "        #forward model\n",
        "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "\n",
        "        #update model\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss = loss.item()\n",
        "        total_train_loss = total_train_loss + tr_loss\n",
        "\n",
        "        #calculate metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "\n",
        "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
        "            total_train_loss/(i+1), get_lr(optimizer)))\n",
        "\n",
        "    #calculate train metric\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
        "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
        "\n",
        "    #evaluate on validation\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    total_loss, total_correct, total_labels = 0,0,0\n",
        "    list_hyp, list_label = [],[]\n",
        "\n",
        "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
        "    for i, batch_data in enumerate(pbar):\n",
        "        batch_seq = batch_data[-1]\n",
        "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "\n",
        "        #calculate total loss\n",
        "        valid_loss = loss.item()\n",
        "        total_loss = total_loss + valid_loss\n",
        "\n",
        "        #calculate evaluation metrics\n",
        "        list_hyp += batch_hyp\n",
        "        list_label += batch_label\n",
        "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "\n",
        "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
        "\n",
        "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
        "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
        "        total_loss/(i+1), metrics_to_string(metrics)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHM16eUkIlBP",
        "outputId": "519dc6c9-bcd5-4dff-8ac8-2f7e4ba0c4a3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "(Epoch 1) TRAIN LOSS:0.3480 LR:0.00000300: 100%|██████████| 344/344 [02:28<00:00,  2.31it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 1) TRAIN LOSS:0.3480 ACC:0.87 F1:0.82 REC:0.79 PRE:0.86 LR:0.00000300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VALID LOSS:0.1947 ACC:0.93 F1:0.90 REC:0.89 PRE:0.90: 100%|██████████| 40/40 [00:07<00:00,  5.33it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Epoch 1) VALID LOSS:0.1947 ACC:0.93 F1:0.90 REC:0.89 PRE:0.90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Epoch 2) TRAIN LOSS:0.1581 LR:0.00000300: 100%|██████████| 344/344 [02:31<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 2) TRAIN LOSS:0.1581 ACC:0.95 F1:0.93 REC:0.93 PRE:0.93 LR:0.00000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VALID LOSS:0.1780 ACC:0.93 F1:0.90 REC:0.91 PRE:0.90: 100%|██████████| 40/40 [00:08<00:00,  4.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 2) VALID LOSS:0.1780 ACC:0.93 F1:0.90 REC:0.91 PRE:0.90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Epoch 3) TRAIN LOSS:0.1184 LR:0.00000300: 100%|██████████| 344/344 [02:32<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 3) TRAIN LOSS:0.1184 ACC:0.96 F1:0.95 REC:0.95 PRE:0.95 LR:0.00000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VALID LOSS:0.1662 ACC:0.94 F1:0.91 REC:0.90 PRE:0.92: 100%|██████████| 40/40 [00:07<00:00,  5.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 3) VALID LOSS:0.1662 ACC:0.94 F1:0.91 REC:0.90 PRE:0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Epoch 4) TRAIN LOSS:0.0881 LR:0.00000300: 100%|██████████| 344/344 [02:33<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 4) TRAIN LOSS:0.0881 ACC:0.97 F1:0.97 REC:0.96 PRE:0.97 LR:0.00000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VALID LOSS:0.1800 ACC:0.93 F1:0.91 REC:0.90 PRE:0.92: 100%|██████████| 40/40 [00:07<00:00,  5.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 4) VALID LOSS:0.1800 ACC:0.93 F1:0.91 REC:0.90 PRE:0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Epoch 5) TRAIN LOSS:0.0656 LR:0.00000300: 100%|██████████| 344/344 [02:33<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 5) TRAIN LOSS:0.0656 ACC:0.98 F1:0.98 REC:0.97 PRE:0.98 LR:0.00000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VALID LOSS:0.2070 ACC:0.93 F1:0.89 REC:0.88 PRE:0.92: 100%|██████████| 40/40 [00:07<00:00,  5.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Epoch 5) VALID LOSS:0.2070 ACC:0.93 F1:0.89 REC:0.88 PRE:0.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will evaluate the test data."
      ],
      "metadata": {
        "id": "21-O2K_zLpiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on test\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "total_loss, total_correct, total_labels = 0,0,0\n",
        "list_hyp, list_label = [],[]\n",
        "\n",
        "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
        "for i, batch_data in enumerate(pbar):\n",
        "    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
        "    list_hyp += batch_hyp\n",
        "\n",
        "#save predicition\n",
        "df = pd.DataFrame({\n",
        "    'label':list_hyp\n",
        "}).reset_index()\n",
        "\n",
        "df.to_csv('pred.txt', index=False)\n",
        "\n",
        "print(df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUdeJoXGLZw7",
        "outputId": "a148ce2a-580e-475b-f79b-e12fec3e460b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:02<00:00,  5.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     index     label\n",
            "0        0  negative\n",
            "1        1  positive\n",
            "2        2  negative\n",
            "3        3  negative\n",
            "4        4  positive\n",
            "..     ...       ...\n",
            "495    495  negative\n",
            "496    496  negative\n",
            "497    497  positive\n",
            "498    498  negative\n",
            "499    499  negative\n",
            "\n",
            "[500 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "5FxWP0OyUo32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(text):\n",
        "    subwords = tokenizer.encode(text)\n",
        "    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "    logits = model(subwords)[0]\n",
        "    label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "    print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ],
      "metadata": {
        "id": "lxgLNWRvUgxU"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
        "prediction(text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "552KYY6XUsPm",
        "outputId": "cf624aed-1127-4828-e7e0-00861a8e9dcf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita | Label : positive (99.592%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"Ronaldo pergi ke Mall Grand Indonesia membeli cilok\"\n",
        "prediction(text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlKoI9jtU5Si",
        "outputId": "1a05c290-85c2-4b0d-883c-eadd0f99830b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Ronaldo pergi ke Mall Grand Indonesia membeli cilok | Label : neutral (98.332%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"Sayang, aku marah\"\n",
        "prediction(text3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTiSMjwRVA5G",
        "outputId": "f076c44d-dca2-4595-96c2-f3b105962be0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Sayang, aku marah | Label : negative (99.763%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4 = \"Merasa kagum dengan toko ini tapi berubah menjadi kecewa setelah transaksi\"\n",
        "prediction(text4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4yjHVMmVIC9",
        "outputId": "35032c04-daf4-4033-e2c1-dd810fdb06fe"
      },
      "execution_count": 61,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Merasa kagum dengan toko ini tapi berubah menjadi kecewa setelah transaksi | Label : negative (99.697%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence on text3 was deliberately made like that with the intention of tricking the machine. The word \"sayang\" (darling) usually has a positive connotation. But in that sentence it is followed by the word \"marah\" (angry).\n",
        "\n",
        "Likewise with text4, \"kagum\" (amazed) and \"kecewa\" (dissapointed) are two words that have positive and negative connotations respectively. If you look at the context of the sentences, it is clear that these two sentences have a negative sentiment. Humans can immediately recognize it as a negative sentiment."
      ],
      "metadata": {
        "id": "WIEkzzF5VU9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text5 = \"Aku padahal bangga banget sama kamu, tapi aku kecewa kenapa kamu memilih jalan yang salah pada akhirnya\"\n",
        "prediction(text5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pixs7IsNVObg",
        "outputId": "27bbb946-6288-4928-8243-565a1d6269f8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Aku padahal bangga banget sama kamu, tapi aku kecewa kenapa kamu memilih jalan yang salah pada akhirnya | Label : negative (99.759%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lth7HmdIdQVs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}